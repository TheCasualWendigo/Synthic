from torch import nn

TRAIN_MARKED_TO_NEXT_SAMPLE = False

ATTENTION_WINDOW_SIZE = 512
ATTENTION_WINDOW_LOOKBACK = 4
ATTENTION_DROPOUT = 0.2

START_LR = 1e-10
MAX_LR = 2e-5
WARMUP_PERIOD = 4
L2_REG_WEIGHT_DECAY = 0.004
EARLY_LOSS_EXIT_LOOKBACK = 3
EARLY_LOSS_EXIT = 2
REDUCE_LR_PATIENCE = 1
REDUCE_LR_FACTOR = 0.9
STEP_SIZE = 8
STEP_GAMMA = 0.9

NUM_ATTENTION_LAYERS = 4
MODEL_DIM = 256
FEED_FORWARD_HDIM = 4

WINDOW_SIZE = 512
ACTIVATION = nn.ReLU  # TODO: Play with different activations?

BATCH_SIZE = 8
