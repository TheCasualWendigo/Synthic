from torch import nn

TRAIN_MARKED_TO_NEXT_SAMPLE = False

ATTENTION_WINDOW_SIZE = 256
ATTENTION_WINDOW_LOOKBACK = 4
ATTENTION_DROPOUT = 0.1

START_LR = 1e-10
MAX_LR = 2e-5
WARMUP_PERIOD = 7
L2_REG_WEIGHT_DECAY = 0.004
EARLY_LOSS_EXIT_LOOKBACK = 2
EARLY_LOSS_EXIT = 2
REDUCE_LR_PATIENCE = 1
REDUCE_LR_FACTOR = 0.9
# Set high because not in use at the moment
STEP_SIZE = 1_000_000
STEP_GAMMA = 0.9

NUM_ATTENTION_LAYERS = 7
MODEL_DIM = 512
FEED_FORWARD_HDIM = 4

WINDOW_SIZE = 128
ACTIVATION = nn.GELU  # TODO: Play with different activations?

BATCH_SIZE = 4
