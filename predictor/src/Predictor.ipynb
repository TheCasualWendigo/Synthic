{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ec8ef4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pescador\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import math\n",
    "\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a429bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGGER = logging.getLogger('gbsd')\n",
    "LOGGER.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b16da3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c20fb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.use('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdbf4a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "cpu = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e88c156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CMD_VOLENVPER = 0\n",
    "CMD_DUTYLL = 1\n",
    "CMD_MSB = 2\n",
    "CMD_LSB = 3\n",
    "CMD_COUNT = 4\n",
    "\n",
    "def onehot_cmd(data):\n",
    "    cmd = data[CMD_OFFSET]\n",
    "    nd = [ 0, 0, 0, 0 ]\n",
    "    nd[int(cmd)] = 1\n",
    "    return nd\n",
    "\n",
    "CH_COUNT = 2\n",
    "\n",
    "TIME_OFFSET = 0\n",
    "CH_OFFSET = 1\n",
    "CMD_OFFSET = 2\n",
    "PARAM1_OFFSET = 3\n",
    "PARAM2_OFFSET = 4\n",
    "PARAM3_OFFSET = 5\n",
    "SIZE_OF_INPUT_FIELDS = 6\n",
    "\n",
    "MAX_WINDOW_SIZE = 4096\n",
    "\n",
    "M_CYCLES_PER_SECOND = 4194304.\n",
    "NORMALIZE_TIME_BY = M_CYCLES_PER_SECOND * 10.\n",
    "\n",
    "def norm(val, max_val):\n",
    "    if val > max_val:\n",
    "        return 1.\n",
    "    else:\n",
    "        return ((val / max_val) * 2.) - 1.\n",
    "\n",
    "def unnorm(val, max_val):\n",
    "    return ((val + 1.) / 2.) * max_val\n",
    "\n",
    "def fresh_input(command, channel, time):\n",
    "    newd = np.zeros(shape=SIZE_OF_INPUT_FIELDS, dtype=int)\n",
    "    newd[TIME_OFFSET] = time\n",
    "    newd[CH_OFFSET] = channel\n",
    "    newd[CMD_OFFSET] = command\n",
    "    return newd\n",
    "\n",
    "def parse_bool(v):\n",
    "    if v == \"true\":\n",
    "        return 1\n",
    "    elif v == \"false\":\n",
    "        return 0\n",
    "    else:\n",
    "        return int(v)\n",
    "\n",
    "def command_of_parts(command, channel, parts, time):\n",
    "    inp = fresh_input(command, channel, time)\n",
    "    \n",
    "    if command == CMD_DUTYLL:\n",
    "        inp[PARAM1_OFFSET] = int(parts[3])\n",
    "        inp[PARAM2_OFFSET] = int(parts[4])\n",
    "    elif command == CMD_VOLENVPER:\n",
    "        inp[PARAM1_OFFSET] = int(parts[3])\n",
    "        inp[PARAM2_OFFSET] = parse_bool(parts[4])\n",
    "        inp[PARAM3_OFFSET] = int(parts[4])\n",
    "    elif command == CMD_LSB:\n",
    "        inp[PARAM1_OFFSET] = int(parts[3])\n",
    "        inp[PARAM2_OFFSET] = 0\n",
    "        inp[PARAM3_OFFSET] = 0\n",
    "    elif command == CMD_MSB:\n",
    "        inp[PARAM1_OFFSET] = int(parts[3])\n",
    "        inp[PARAM2_OFFSET] = parse_bool(parts[4])\n",
    "        inp[PARAM3_OFFSET] = parse_bool(parts[5])\n",
    "    else:\n",
    "        raise \"this should not happen\"\n",
    "    return inp\n",
    "\n",
    "def int32_as_bytes(ival):\n",
    "    return np.frombuffer(ival.item().to_bytes(4, byteorder = 'big'), dtype=np.uint8)\n",
    "\n",
    "def int32_of_bytes(np):\n",
    "    return int.from_bytes(np, byteorder = 'big')\n",
    "\n",
    "def int8_as_bytes(ival):\n",
    "    return np.frombuffer(ival.item().to_bytes(1, byteorder='big'), dtype=np.uint8)\n",
    "\n",
    "def int8_of_bytes(np):\n",
    "    return int.from_bytes(np, byteorder = 'big')\n",
    "\n",
    "def merge_params(data):\n",
    "    command = data[CMD_OFFSET]\n",
    "    if command == CMD_DUTYLL:\n",
    "        return (data[PARAM1_OFFSET] << 6) | data[PARAM2_OFFSET]\n",
    "    elif command == CMD_VOLENVPER:\n",
    "        return (data[PARAM1_OFFSET] << 4) | (data[PARAM2_OFFSET] << 3) | data[PARAM3_OFFSET]\n",
    "    elif command == CMD_LSB:\n",
    "        return data[PARAM1_OFFSET]\n",
    "    elif command == CMD_MSB:\n",
    "        return data[PARAM1_OFFSET]  | (data[PARAM2_OFFSET] << 6) | (data[PARAM3_OFFSET] << 7)\n",
    "    else:\n",
    "        raise \"this should not happen\"\n",
    "        \n",
    "def unmerge_params(command, data,v ):\n",
    "    if command == CMD_DUTYLL:\n",
    "        data[PARAM1_OFFSET] = v >> 6;\n",
    "        data[PARAM2_OFFSET] = v & 0b0011_1111\n",
    "    elif command == CMD_VOLENVPER:\n",
    "        data[PARAM1_OFFSET] = v >> 4\n",
    "        data[PARAM2_OFFSET] = (v & 0b0000_1000) >> 3\n",
    "        data[PARAM3_OFFSET] = (v & 0b0000_0111)\n",
    "    elif command == CMD_LSB:\n",
    "        data[PARAM1_OFFSET] = v\n",
    "    elif command == CMD_MSB:\n",
    "        data[PARAM1_OFFSET] = v & 0b0011_1111\n",
    "        data[PARAM2_OFFSET] = (v & 0b0100_0000) >> 6\n",
    "        data[PARAM3_OFFSET] = (v & 0b1000_0000) >> 7\n",
    "    else:\n",
    "        raise Exception(\"this should not happen\")\n",
    "        \n",
    "BYTES_PER_ENTRY=7\n",
    "\n",
    "def command_to_bytes(command):\n",
    "    new_arr = np.concatenate([\n",
    "                    int32_as_bytes(command[TIME_OFFSET]),\n",
    "                    int8_as_bytes(command[CH_OFFSET]),\n",
    "                    int8_as_bytes(command[CMD_OFFSET]),\n",
    "                    int8_as_bytes(merge_params(command)),]).flatten()\n",
    "    return new_arr\n",
    "\n",
    "def command_of_bytes(byte_arr):\n",
    "    d = fresh_input(0, 0, 0)\n",
    "    d[TIME_OFFSET] = int32_of_bytes(byte_arr[0:4])\n",
    "    print(byte_arr[0:4], d[TIME_OFFSET])\n",
    "    d[TIME_OFFSET] = min(d[TIME_OFFSET], 400000)\n",
    "    print(\"TOFF:\", d[TIME_OFFSET])\n",
    "    print(\"CH:\", byte_arr, byte_arr[4])\n",
    "    d[CH_OFFSET] = int8_of_bytes(byte_arr[4:5])\n",
    "    if d[CH_OFFSET] != 1 and d[CH_OFFSET] != 2:\n",
    "        raise Exception(\"bad channel prediction\")\n",
    "    d[CMD_OFFSET] = int8_of_bytes(byte_arr[5:6])\n",
    "    print(\"NCMD\", d[CMD_OFFSET])\n",
    "    unmerge_params(d[CMD_OFFSET], d, byte_arr[6])\n",
    "    return d\n",
    "\n",
    "def unnorm_feature(data):\n",
    "    \n",
    "    data = data.copy()\n",
    "    \n",
    "    # Unnormalize a channel given a specific max value\n",
    "    def l_unnorm(channel, maxv):\n",
    "        data[channel] = unnorm(data[channel], maxv)\n",
    "    \n",
    "    # Round and box a channel to a min and max value\n",
    "    def l_round(channel, minv, maxv):\n",
    "        data[channel] = round(min(max(data[channel], minv), maxv))\n",
    "    \n",
    "    l_unnorm(TIME_OFFSET, NORMALIZE_TIME_BY)\n",
    "    \n",
    "    if data[TIME_OFFSET] < 4:\n",
    "        raise Exception('bad time prediction')\n",
    "    \n",
    "    l_unnorm(CH_OFFSET, CH_COUNT)\n",
    "    data[CH_OFFSET] = round(data[CH_OFFSET])\n",
    "    \n",
    "    l_unnorm(CMD_OFFSET, CMD_COUNT)\n",
    "    data[CMD_OFFSET] = round(data[CMD_OFFSET])\n",
    "    \n",
    "    command = data[CMD_OFFSET]\n",
    "    \n",
    "    if command == CMD_DUTYLL:\n",
    "        l_unnorm(PARAM1_OFFSET, 2)\n",
    "        l_round(PARAM1_OFFSET, 0, 2)\n",
    "        \n",
    "        l_unnorm(PARAM2_OFFSET, 64)\n",
    "        l_round(PARAM2_OFFSET, 0, 64)\n",
    "        \n",
    "        data[PARAM3_OFFSET] = 0.\n",
    "    elif command == CMD_VOLENVPER:\n",
    "        \n",
    "        l_unnorm(PARAM1_OFFSET, 16)\n",
    "        l_round(PARAM1_OFFSET, 0, 16)\n",
    "        \n",
    "        l_round(PARAM2_OFFSET, 0, 1)\n",
    "        \n",
    "        l_unnorm(PARAM3_OFFSET, 7)\n",
    "        l_round(PARAM3_OFFSET, 0, 7)\n",
    "    elif command == CMD_LSB:\n",
    "        \n",
    "        l_unnorm(PARAM1_OFFSET, 255.)\n",
    "        l_round(PARAM1_OFFSET, 0., 255.)\n",
    "        \n",
    "        data[PARAM2_OFFSET] = 0.\n",
    "        \n",
    "        data[PARAM3_OFFSET] = 0.\n",
    "    elif command == CMD_MSB:\n",
    "        l_unnorm(PARAM1_OFFSET, 7.)\n",
    "        l_round(PARAM1_OFFSET, 0., 7.)\n",
    "        \n",
    "        l_round(PARAM2_OFFSET, 0., 1.)\n",
    "        l_round(PARAM3_OFFSET, 0., 1.)\n",
    "    else:\n",
    "        raise Exception(\"pred was bad\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def print_feature(data, file=sys.stdout):\n",
    "    \n",
    "    print(\"FILE: \", file)\n",
    "    print(\"FORPRINT\", data)\n",
    "\n",
    "    command = data[CMD_OFFSET]\n",
    "    \n",
    "    if command == CMD_DUTYLL:\n",
    "        print(f\"CH {data[CH_OFFSET]} DUTYLL {data[PARAM1_OFFSET]} {data[PARAM2_OFFSET]} AT {data[TIME_OFFSET]}\", file=file, flush=True)\n",
    "    elif command == CMD_VOLENVPER:\n",
    "        print(f\"CH {data[CH_OFFSET]} VOLENVPER {data[PARAM1_OFFSET]} {data[PARAM2_OFFSET]} {data[PARAM3_OFFSET]} AT {data[TIME_OFFSET]}\", file=file, flush=True)\n",
    "    elif command == CMD_LSB:\n",
    "        print(f\"CH {data[CH_OFFSET]} FREQLSB {data[PARAM1_OFFSET]} AT {data[TIME_OFFSET]}\", file=file, flush=True)\n",
    "    elif command == CMD_MSB:\n",
    "        print(f\"CH {data[CH_OFFSET]} FREQMSB {data[PARAM1_OFFSET]} {data[PARAM2_OFFSET]} {data[PARAM3_OFFSET]} AT {data[TIME_OFFSET]}\", file=file, flush=True)\n",
    "    else:\n",
    "        print(f\"Bad prediction\", file=file, flush=True)\n",
    "\n",
    "def load_training_data(src):\n",
    "    data = []\n",
    "    file = open(src, 'r')\n",
    "    for line in file:\n",
    "        parts = line.split()\n",
    "        if len(parts) > 0 and parts[0] == \"CH\":\n",
    "            #print(parts)\n",
    "            channel = int(parts[1])\n",
    "            command = parts[2]\n",
    "            time = int(parts[-1])\n",
    "            if command == \"DUTYLL\":\n",
    "                new_item = command_of_parts(CMD_DUTYLL, channel, parts, time)\n",
    "            elif command == \"VOLENVPER\":\n",
    "                new_item = command_of_parts(CMD_VOLENVPER, channel, parts, time)\n",
    "            elif command == \"FREQLSB\":\n",
    "                new_item = command_of_parts(CMD_LSB, channel, parts, time)\n",
    "            elif command == \"FREQMSB\":\n",
    "                new_item = command_of_parts(CMD_MSB, channel, parts, time)\n",
    "            else:\n",
    "                print(\"Unknown\", command)\n",
    "             # Otherwise unknown   \n",
    "            data.append(new_item)\n",
    "    return data\n",
    "\n",
    "@pescador.streamable\n",
    "def samples_from_training_data(src, window_size):\n",
    "    sample_data = None\n",
    "    try:\n",
    "        sample_data = load_training_data(src)\n",
    "    except Exception as e:\n",
    "        LOGGER.error('Could not load {}: {}'.format(src, str(e)))\n",
    "        raise StopIteration()\n",
    "\n",
    "    while True:\n",
    "        if len(sample_data) < window_size:\n",
    "            sample = sample_data\n",
    "        else:\n",
    "            # Sample a random window from the audio file\n",
    "            start_idx = np.random.randint(0, len(sample_data) - window_size)\n",
    "            sample = sample_data[start_idx:(start_idx + window_size)]\n",
    "            \n",
    "        sample = np.array([command_to_bytes(x) for x in sample])\n",
    "\n",
    "        yield { 'X':sample }\n",
    "\n",
    "def create_batch_generator(paths, window_size):\n",
    "    streamers = []\n",
    "    for path in paths:\n",
    "        streamers.append(samples_from_training_data(path, window_size))\n",
    "    mux = pescador.StochasticMux(streamers, n_active=1, rate=1).iterate()\n",
    "    \n",
    "    return mux\n",
    "\n",
    "def training_files(dirp):\n",
    "    return [\n",
    "      os.path.join(root, fname)\n",
    "      for (root, dir_names, file_names) in os.walk(dirp, followlinks=True)\n",
    "      for fname in file_names\n",
    "    ]\n",
    "\n",
    "def create_data_split(paths, window_size=MAX_WINDOW_SIZE):\n",
    "    train_gen = create_batch_generator(paths, window_size)\n",
    "    return train_gen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45bb8c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting training data\n",
      "Collected\n"
     ]
    }
   ],
   "source": [
    "print(\"Collecting training data\")\n",
    "train_gen = create_data_split(training_files(\"../..//training_data/\"))\n",
    "print(\"Collected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b714610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.gamma import Gamma\n",
    "from torch.distributions.normal import Normal\n",
    "import torch\n",
    "\n",
    "class NIGDist():\n",
    "    \n",
    "    def __init__(self, m, vinv, a, b):\n",
    "        self.m = m\n",
    "        self.vinv = vinv\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "    def update(self, total, moment, n):\n",
    "        \n",
    "        newVinv = self.vinv  + n\n",
    "        newM = (1.0 / newVinv) * (self.vinv * self.m + total)\n",
    "        self.a += n / 2\n",
    "        self.b += 0.5 * (self.m * self.m * self.vinv + moment - newM * newM * newVinv)\n",
    "        self.m = newM\n",
    "        self.vinv = newVinv\n",
    "        \n",
    "        \n",
    "    def update(self, x):\n",
    "        total = torch.sum(x, dim=1)\n",
    "        moment = torch.sum(x * x, dim=1)\n",
    "        n = x.shape[1]\n",
    "        self.update(total, moment, n)\n",
    "\n",
    "    def sample(self):\n",
    "        vars = 1.0 / (Gamma(self.a, self.b)).sample()\n",
    "        means = Normal(self.m, vars / self.vinv).sample()\n",
    "        return means, vars\n",
    "\n",
    "    def update_component(self, index, x):\n",
    "        total = torch.sum(x)\n",
    "        moment = torch.sum(x * x)\n",
    "        n = x.shape[0]\n",
    "        newVinv = self.vinv[index]  + n\n",
    "        newM = (1.0 / newVinv) * (self.vinv[index] * self.m[index] + total)\n",
    "        self.a[index] += n / 2\n",
    "        self.b[index] += 0.5 * (self.m[index] * self.m[index] * self.vinv[index] + moment - newM * newM * newVinv)\n",
    "        self.m[index] = newM\n",
    "        self.vinv[index] = newVinv\n",
    "\n",
    "def standardNIG(batch, device):\n",
    "    return NIGDist(torch.zeros(batch, device=device), torch.ones(batch, device=device), torch.ones(batch, device=device), torch.ones(batch, device=device))\n",
    "\n",
    "class ThompsonTuner():\n",
    "    def __init__(self, stepper, num_options, device):\n",
    "        self.stepper = stepper\n",
    "        self.arms = num_options        \n",
    "        self.belief = standardNIG(self.arms, device)\n",
    "        self.histogram = torch.zeros(self.arms, device=device)\n",
    "        self.n = 0\n",
    "        self.ngpu = torch.zeros(1, device=device)\n",
    "        self.device = device\n",
    "\n",
    "    def step(self):\n",
    "        samples, _ = self.belief.sample()\n",
    "        play = torch.argmax(samples)\n",
    "        obs = self.stepper(play)\n",
    "        self.belief.update_component(play, obs)\n",
    "        self.histogram[play] += 1\n",
    "        self.n += 1\n",
    "        self.ngpu += 1\n",
    "        if self.arms == 2:\n",
    "            pass # Special analytic case\n",
    "        if self.n >= 5:\n",
    "            #print(torch.max(self.histogram) / self.ngpu)\n",
    "            if (torch.max(self.histogram) / self.ngpu) > 0.80:\n",
    "                choice = torch.argmax(self.histogram)\n",
    "                self.stepper.choose(choice)\n",
    "                self.ngpu.zero_()\n",
    "                self.n = 0\n",
    "                self.histogram.zero_()\n",
    "                self.belief.m = self.belief.m[choice].repeat(self.arms)\n",
    "                self.belief.vinv = self.belief.vinv[choice].repeat(self.arms)\n",
    "                self.belief.a = self.belief.a[choice].repeat(self.arms)\n",
    "                self.belief.b = self.belief.b[choice].repeat(self.arms)\n",
    "                self.belief.vinv /= 2\n",
    "                self.belief.a /= 2\n",
    "                self.belief.b /= 2\n",
    "\n",
    "class LRTuner():\n",
    "    #Note: Stepper produces correlated outputs. Take two steps per tuning step to avoid? (Overlapping minibatches?)\n",
    "    class Stepper():\n",
    "        def __init__(self, base):\n",
    "            self.base = base\n",
    "            \n",
    "        def __call__(self, choice):\n",
    "            return self.base.opt(self.base.LRs[choice])\n",
    "            \n",
    "\n",
    "        def choose(self, choice):\n",
    "            #print(\"chose!\")\n",
    "            ratio = self.base.ratio\n",
    "            if choice == 0:\n",
    "                self.base.LRs /= ratio\n",
    "            else:\n",
    "                self.base.LRs *= ratio\n",
    "            \n",
    "    def __init__(self, lr, opt, device, ratio=1.259921049):\n",
    "        self.opt = opt\n",
    "        self.ratio = ratio\n",
    "        self.LRs = torch.tensor([lr, lr * ratio], device=device)\n",
    "        self.tuner = ThompsonTuner(LRTuner.Stepper(self), 2, device)\n",
    "\n",
    "    def step(self):\n",
    "        self.tuner.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb0f0e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish(x, b):\n",
    "    return x * torch.sigmoid(b * x)\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self, chan):\n",
    "        super(Swish, self).__init__()\n",
    "        self.register_parameter('weight', nn.Parameter(torch.ones(chan)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return swish(x, self.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "526c9038",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout = 0.1, max_len = MAX_WINDOW_SIZE):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "deaa85e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Functional(nn.Module):\n",
    "    def __init__(self, f):\n",
    "        super(Functional, self).__init__()\n",
    "        self.f = f\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c61214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.2088217163085937\n",
      "LR: 0.009999999776482582\n",
      "Saving checkpoint\n",
      "Saved checkpoint\n",
      "Loss: 2.304729461669922\n",
      "LR: 0.009999999776482582\n",
      "Saving checkpoint\n",
      "Saved checkpoint\n",
      "Loss: 2.0896235656738282\n",
      "LR: 0.009999999776482582\n",
      "Saving checkpoint\n",
      "Saved checkpoint\n",
      "Loss: 1.960494384765625\n",
      "LR: 0.009999999776482582\n",
      "Saving checkpoint\n",
      "Saved checkpoint\n",
      "Loss: 1.9032095336914063\n",
      "LR: 0.009999999776482582\n",
      "Saving checkpoint\n",
      "Saved checkpoint\n",
      "Loss: 1.8320292663574218\n",
      "LR: 0.009999999776482582\n",
      "Saving checkpoint\n",
      "Saved checkpoint\n",
      "Loss: 1.7759666442871094\n",
      "LR: 0.009999999776482582\n",
      "Saving checkpoint\n",
      "Saved checkpoint\n",
      "Loss: 1.7249778747558593\n",
      "LR: 0.009999999776482582\n",
      "Saving checkpoint\n",
      "Saved checkpoint\n",
      "Loss: 1.7201437377929687\n",
      "LR: 0.009999999776482582\n",
      "Saving checkpoint\n",
      "Saved checkpoint\n",
      "Loss: 1.7098617553710938\n",
      "LR: 0.009999999776482582\n",
      "Saving checkpoint\n",
      "Saved checkpoint\n",
      "Loss: 1.6526358032226562\n",
      "LR: 0.01259899977594614\n",
      "Saving checkpoint\n",
      "Saved checkpoint\n",
      "Loss: 1.5716722106933594\n",
      "LR: 0.01259899977594614\n",
      "Saving checkpoint\n",
      "Saved checkpoint\n",
      "Loss: 1.6319915771484375\n",
      "LR: 0.01259899977594614\n",
      "Saving checkpoint\n",
      "Saved checkpoint\n",
      "Loss: 1.6749853515625\n",
      "LR: 0.01259899977594614\n",
      "Saving checkpoint\n",
      "Saved checkpoint\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 500000\n",
    "ROUND_SZ = 100\n",
    "\n",
    "class CausalConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation=1, **kwargs):\n",
    "        super(CausalConv1d, self).__init__()\n",
    "        self.pad = (kernel_size - 1) * dilation\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation = dilation\n",
    "\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size , dilation=dilation, **kwargs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #pad here to only add to the left side\n",
    "        x = F.pad(x, (self.pad, 0))\n",
    "        return self.conv(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, kernel_size, skip_channels, dilation=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.dilation = dilation\n",
    "        self.conv_sig = CausalConv1d(input_channels, output_channels, kernel_size, dilation)#dim\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.conv_tan = CausalConv1d(input_channels, output_channels, kernel_size, dilation)#dim\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        #separate weights for residual and skip channels\n",
    "        self.conv_r = nn.Conv1d(output_channels, output_channels, 1)#dim -> k = 1\n",
    "        self.conv_s = nn.Conv1d(output_channels, skip_channels, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        o = self.sig(self.conv_sig(x)) * self.tanh(self.conv_tan(x))\n",
    "        skip = self.conv_s(o)\n",
    "        residual = self.conv_r(o)\n",
    "        return residual, skip\n",
    "\n",
    "class WaveNet(nn.Module):\n",
    "    def __init__(self, skip_channels=256, num_blocks=3, num_layers=12, num_hidden=128, kernel_size=2): \n",
    "        super(WaveNet, self).__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(skip_channels, skip_channels)\n",
    "        self.positional_embedding = PositionalEncoding(skip_channels)\n",
    "        self.causal_conv = CausalConv1d(skip_channels, num_hidden, kernel_size)\n",
    "        self.res_stack = nn.ModuleList()\n",
    "\n",
    "        for b in range(num_blocks):\n",
    "            for i in range(num_layers):\n",
    "                self.res_stack.append(ResidualBlock(num_hidden, num_hidden, kernel_size, skip_channels=skip_channels, dilation=2**i))\n",
    "        \n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv1 = nn.Conv1d(skip_channels, skip_channels, 1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(skip_channels, skip_channels, 1)\n",
    "        #self.output = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        o = self.embed(x)\n",
    "        o = self.positional_embedding(o)\n",
    "        o = o.permute(0,2,1)\n",
    "\n",
    "        skip_vals = []\n",
    "        #initial causal conv\n",
    "        o = self.causal_conv(o)\n",
    "        \n",
    "        #run res blocks\n",
    "        for i, layer in enumerate(self.res_stack):\n",
    "            o, s = layer(o)\n",
    "            skip_vals.append(s)\n",
    "            \n",
    "        #sum skip values and pass to last portion of network\n",
    "        o = reduce((lambda a,b: a+b), skip_vals)\n",
    "        o = self.relu1(o)\n",
    "        o = self.conv1(o)\n",
    "        o = self.relu2(o)\n",
    "        o = self.conv2(o)\n",
    "        \n",
    "        return o #self.output(o)\n",
    "\n",
    "KERNEL_SIZE_SAMPLES=16\n",
    "KERNEL_SIZE=BYTES_PER_ENTRY * KERNEL_SIZE_SAMPLES\n",
    "    \n",
    "def command_net():\n",
    "    return WaveNet(kernel_size=KERNEL_SIZE)\n",
    "\n",
    "def load(path):\n",
    "    command_generator = command_net()\n",
    "    \n",
    "    if path != None:\n",
    "        command_generator.load_state_dict(torch.load(path))\n",
    "        \n",
    "    command_generator = command_generator.to(device)\n",
    "\n",
    "    return command_generator\n",
    "\n",
    "def train(path, baseOpt=optim.SGD, tune_ratio=1.2599, device=torch.device(0)):\n",
    "    \n",
    "    lr = 0.01\n",
    "    momentum=0.85\n",
    "    \n",
    "    command_generator = load(path)\n",
    "\n",
    "    baseOpt = baseOpt(command_generator.parameters(), lr=lr, momentum=momentum)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    running_loss = torch.zeros(1, device=device)\n",
    "    last_loss = torch.zeros(1, device=device)\n",
    "\n",
    "    def step(lr):\n",
    "        \n",
    "        for i in range(ROUND_SZ):\n",
    "            ntrain =  next(train_gen)['X'].flatten().copy()\n",
    "            seq = torch.Tensor(ntrain).long().to(device)\n",
    "            inputs = seq[:-1].unsqueeze(0)\n",
    "            labels = seq[1:].unsqueeze(0)\n",
    "    \n",
    "            baseOpt.zero_grad()\n",
    "\n",
    "            for g in baseOpt.param_groups:\n",
    "                g['lr'] = lr\n",
    "\n",
    "            outputs = command_generator(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            baseOpt.step()\n",
    "            running_loss.add_(loss.detach())\n",
    "            \n",
    "            if i == 0:\n",
    "                last_loss.copy_(loss.detach())\n",
    "            result = last_loss - loss.detach()\n",
    "            \n",
    "            seq = seq.detach().to(cpu)\n",
    "            del inputs\n",
    "            del labels\n",
    "            del seq\n",
    "\n",
    "        result = running_loss / ROUND_SZ\n",
    "        return result\n",
    "\n",
    "    opt = LRTuner(lr, step, device, tune_ratio) \n",
    "\n",
    "    for i in range(0, EPOCHS):\n",
    "        opt.step()\n",
    "\n",
    "        print(\"Loss:\", running_loss.item() / ROUND_SZ)\n",
    "        running_loss.zero_()\n",
    "        print(\"LR:\",opt.LRs[0].item())\n",
    "        \n",
    "        print(\"Saving checkpoint\")\n",
    "        torch.save(command_generator.state_dict(), \"./\" + str(int(datetime.now().timestamp())) + \".checkpoint.model\")\n",
    "        torch.save(command_generator.state_dict(), \"./last.checkpoint.model\")\n",
    "        print(\"Saved checkpoint\")\n",
    "        \n",
    "        gc.collect()\n",
    "    \n",
    "    return command_generator.eval()\n",
    "\n",
    "train(\"../../training_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c18ca04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%capture cap --no-stderr\n",
    "\n",
    "print(\"Collecting training data\")\n",
    "train_gen = create_data_split(training_files(\"../../training_data/\"))\n",
    "print(\"Collected\")\n",
    "\n",
    "command_generator = load(\"./last.checkpoint.model\").eval()\n",
    "\n",
    "seed = next(train_gen)['X'].flatten().copy()\n",
    "\n",
    "def max_of(v, begin, end):\n",
    "    return begin + np.argmax(v[begin:end])\n",
    "\n",
    "with open('output.txt', 'w') as f:\n",
    "\n",
    "    for i in range(0, len(seed), BYTES_PER_ENTRY):\n",
    "        print(\"Seed value :\", i)\n",
    "        cmd = command_of_bytes(seed[i:i+BYTES_PER_ENTRY])\n",
    "        print_feature(cmd)\n",
    "        print_feature(cmd, file=f)\n",
    "    \n",
    "    for i in range(BYTES_PER_ENTRY * 10000):\n",
    "        seq = torch.Tensor(seed).long().to(device).unsqueeze(0)\n",
    "        pred = command_generator(seq).detach().to(cpu).permute(0,2,1).squeeze(0).numpy()\n",
    "        new_pred_bytes = np.array([np.argmax(x) for x in pred[-1:]]).astype(np.uint8)\n",
    "        seed = np.concatenate([seed[1:], new_pred_bytes])\n",
    "\n",
    "        print(\"Step: \", i)\n",
    "    \n",
    "        if (i + 1) % BYTES_PER_ENTRY == 0:\n",
    "            try:\n",
    "                print(\"New pred bytes:\", seed[-7:])\n",
    "                new_pred = command_of_bytes(seed[-7:])\n",
    "                print_feature(new_pred)\n",
    "                print_feature(new_pred, file=f)\n",
    "            except BaseException as err:\n",
    "                print(\"pred was not valid because:\", err)\n",
    "\n",
    "    del pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
